:toc:

= Terraform Workshop

This workshop builds heavily on the great book "Terraform Up & Running" by Yevgeniy (Jim) Brikman, which I highly recommend reading.

* https://www.terraform.io[terraform.io]: "Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files."
* written in "HashiCorp Configuration Language" (HCL) in file with .tf extension
* support to cloud vendor via https://registry.terraform.io/browse/providers[providers], which have to be downloaded from the https://registry.terraform.io[registry] when used
* example: https://registry.terraform.io/providers/hashicorp/aws/latest/docs[Using AWS Provider]
* benefit of Terraform: syntax for each provider is the same, hence not the specifics of a provider have to be learned, they are abstracted away by Terraform scripts that look the same for each provider.

== Creating First Resources

Resource syntax:

[source,hcl-terraform]
----
resource "<provider>_<resource_type>" "name" {
  config options
}
----

* _<provider>_ will be "aws" if AWS is to be targeted
* _<resource_type>_ can also be found in the https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ami[documentation]

For example:

.01-create-ec2/main.tf
[source,hcl-terraform]
----
# Create a VPC
resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}
----

Or:

.01-create-ec2/main.tf
[source,hcl-terraform]
----
# Create an EC2
resource "aws_instance" "my-instance" {
  # Ubuntu Server 20.04 LTS
  ami = "ami-0d527b8c289b4af7f"
  instance_type = "t2.micro"
}
----

**AMI-IDs are region-specific! When copy-pasting an ID from the web console, make sure to select the right region.**

In the directory where the above file is located, Terraform has to be initiated by calling _terraform init_ to

* download all the necessary providers into the folder _.terraform_
* configure the backend (see next sections)
* downloading modules (see next sections)

[source,terminal]
----
$ terraform init

Initializing the backend...

Initializing provider plugins...
- Finding hashicorp/aws versions matching "~> 3.0"...
- Installing hashicorp/aws v3.74.3...
- Installed hashicorp/aws v3.74.3 (signed by HashiCorp)

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
----

The command _terraform init_ is idempotent, it can be run multiple times in the same directory.

**The automatically created file .terraform.lock.hcl should be added to version control!**

**The automatically created file terraform.tfstate is needed to manage the state and must not be deleted. Also, in includes sensitive information and should not be put in version control!**

Validate configuration with

[source,terminal]
----
$ terraform validate
Success! The configuration is valid.
----

List of all resources to be created with

[source,terminal]
----
terraform plan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following
symbols:
  + create

Terraform will perform the following actions:

  # aws_instance.my-instance will be created
  + resource "aws_instance" "my-instance" {
      + ami                                  = "ami-0d527b8c289b4af7f"
      + arn                                  = (known after apply)
      + associate_public_ip_address          = (known after apply)
      + availability_zone                    = (known after apply)
      + cpu_core_count                       = (known after apply)
      + cpu_threads_per_core                 = (known after apply)
      + disable_api_termination              = (known after apply)
      + ebs_optimized                        = (known after apply)
      + get_password_data                    = false
      + host_id                              = (known after apply)
      + id                                   = (known after apply)
      + instance_initiated_shutdown_behavior = (known after apply)
      + instance_state                       = (known after apply)
      + instance_type                        = "t2.micro"
      + ipv6_address_count                   = (known after apply)
      + ipv6_addresses                       = (known after apply)
      + key_name                             = (known after apply)
      + monitoring                           = (known after apply)
      + outpost_arn                          = (known after apply)
      + password_data                        = (known after apply)
      + placement_group                      = (known after apply)
      + placement_partition_number           = (known after apply)
      + primary_network_interface_id         = (known after apply)
      + private_dns                          = (known after apply)
      + private_ip                           = (known after apply)
      + public_dns                           = (known after apply)
      + public_ip                            = (known after apply)
      + secondary_private_ips                = (known after apply)
      + security_groups                      = (known after apply)
      + source_dest_check                    = true
      + subnet_id                            = (known after apply)
      + tags_all                             = (known after apply)
      + tenancy                              = (known after apply)
      + user_data                            = (known after apply)
      + user_data_base64                     = (known after apply)
      + vpc_security_group_ids               = (known after apply)

      + capacity_reservation_specification {
          + capacity_reservation_preference = (known after apply)

          + capacity_reservation_target {
              + capacity_reservation_id = (known after apply)
            }
        }

      + ebs_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + snapshot_id           = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }

      + enclave_options {
          + enabled = (known after apply)
        }

      + ephemeral_block_device {
          + device_name  = (known after apply)
          + no_device    = (known after apply)
          + virtual_name = (known after apply)
        }

      + metadata_options {
          + http_endpoint               = (known after apply)
          + http_put_response_hop_limit = (known after apply)
          + http_tokens                 = (known after apply)
          + instance_metadata_tags      = (known after apply)
        }

      + network_interface {
          + delete_on_termination = (known after apply)
          + device_index          = (known after apply)
          + network_interface_id  = (known after apply)
        }

      + root_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run "terraform
apply" now.
----

Create resources specified in current folder with:

[source,terminal]
----
$ terraform apply
----

Multiple executions of _terraform apply_ will not create multiple resources because of the names of the created resources and the declarative approach.

After modifications, run _terraform plan_ and after that _terraform apply_.

== Destroying Resources

To destroy all resources defined in the files in the current directory and created by those resources, run:

[source,terminal]
----
$ terraform destroy
----

Also, resources existing in AWS but not defined in Terraform will be deleted when executing _Terraform apply_.

== Referencing Resources

References to other resources are done with the <resource_type>.<resource_name>.id like in this example:

.02-referencing-resources/main.tf
[source,hcl-terraform]
----
# Create a VPC and subnet
resource "aws_vpc" "vpc-1" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "production "
  }
}

resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.vpc-1.id
  cidr_block = "10.0.1.0/24"
  tags = {
    Name = "prod-subnet"
  }
}
----

Using the _Name_-tag will allow easy identification of the created resource in the AWS web console because the name of resources will be shown there, if the tag is _Name_ (with upper-case!).

Resources don't need to be declared in a specific order, Terraform figures out what to create first by itself. It creates a dependency graph from all the references that can be printed with:

[source,terminal]
----
$ terraform graph
----

Output of _terraform graph_ is written in DOT and can be converted to images using tools like https://dreampuf.github.io/GraphvizOnline/[Graphviz Online].


== User Data

When starting an EC2 instance, user data can be defined to run some script after creation of the instance.

.03-practice-project/main.tf
[source,hcl-terraform]
----
resource "aws_instance" "web-server-instance" {
  # Ubuntu Server 20.04 LTS
  ami = "ami-04505e74c0741db8d"
  instance_type = "t2.micro"
  availability_zone = "us-east-1a"
  key_name = "main-keypair"

  network_interface {
    device_index = 0
    network_interface_id = aws_network_interface.web-server-nic.id
  }

  user_data = <<-EOF
    #!/bin/bash
    sudo apt update -y
    sudo apt install apache2 -y
    sudo systemctl start apache2
    sudo bash -c 'echo your very first web server > /var/www/html/index.html'
    EOF
  tags = {
    Name = "web-server"
  }
}
----

== Output Variables

Within Terraform script, *output* can be defined like this (values from '_terraform state show aws_some_resource_'):

[source,hcl-terraform]
----
# Print public IP of server after creation
output "server_public_ip" {
  value = aws_eip.aws_eip.public_ip
  description ="The public IP of the server"
}
----

_output_ can be every Terraform expression.

As a best practice, outputs should reside in a file *outputs.tf*.

After _terraform apply_, *all* outputs can be printed again using

[source,terminal]
----
$ terraform output
----

*Certain* outputs can be printed using

[source,terminal]
----
$ terraform output server_public_ip
----

Output variables may contain a configuration with the following parameters:

* description
* sensitive (true, if the output should not be printed at the end of _apply_)
* depends_on (if Terraform cannot figure out the dependencies by itself)

Example for showing public IP address of server:

[source,hcl-terraform]
----
output "public_ip" {
  value = aws_instance.example.public_ip
  description = "The public IP address of the web server"
}
----

== Useful Commands and Settings

To automatically *confirm* changes:

[source,terminal]
----
$ terraform apply --auto-approve
----

*Show current state*:

[source,hcl-terraform]
----
$ terraform show
----

*List* all resources:

[source,terminal]
----
$ terraform state list
----

*Show details* about one of the resources:

[source,terminal]
----
$ terraform state show aws_some_resource
----

To *refresh* all states and run the outputs again (great for printing the IPs mentioned before):

[source,terminal]
----
$ terraform refresh
----

*Targeting* single resources instead of changing all the resources in a script:

[source,terminal]
----
$ terraform destroy -target aws_some_resource
$ terraform apply -target aws_some_resource
----

== Input Variables

To follow the DRY principle, code can be extracted into variables that have the following optional parameters:

* description
* default
* type (for example string, number, bool, list, map, set, object, tuple, any)
* validation (custom validation rules)
* sensitive (true / false; log variable or not)

For example, extract the CIDR block from the following code:

[source,hcl-terraform]
----
resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.prod-vpc.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-east-1a"
  tags = {
    Name = "prod-subnet"
  }
}
----

The following code uses a *variable reference* and will ask for a value for _subnet_prefix_ when performing _terraform apply_:

[source,hcl-terraform]
----
variable "subnet_prefix" {
  description = "cidr block for subnet"
  type = string
}

resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.prod-vpc.id
  cidr_block = var.subnet_prefix
  availability_zone = "us-east-1a"
  tags = {
    Name = "prod-subnet"
  }
}
----

It will also ask for a value when destroying the resource, which doesn't matter so any value can be entered or none at all.

When creating the resources, the value could be assigned via a _default_ field in the definition of the variable.

Alternatively, it can be defined as a command line argument:

[source,terminal]
----
$ terraform apply -var "subnet_prefix=10.0.100.0/24"
----

However, best solution to define variables is via a separate file called *_terraform.tfvars_* in the same directory as the _.tf_ file:

.terraform.tfvars
[source,hcl-terraform]
----
subnet_prefix = "10.0.200.0/24"
----

When creating multiple variable files, the default name _terraform.tfvars_ cannot be used. References to variable files can be assigned with:

[source,terminal]
----
$ terraform apply --var-file example.tfvars
----

Example with combined types:

[source,hcl-terraform]
----
variable "list_numeric_example" {
  description = "An example of a numeric list in Terraform"
  type = list(number)
  default = [1, 2, 3]
}
----

Example with combined, structural types:

[source,hcl-terraform]
----
variable "object_example" {
  description = "An example of a structural type in Terraform"
  type = object({
    name = string
    age = number
    tags = list(string)
    enabled = bool
  })
  default = {
    name = "value1"
    age = 42
    tags = ["a", "b", "c"]
    enabled = true
  }
}
----

Using *interpolation*, a variable can also be used in the user data block:

[source,hcl-terraform]
----
user_data = <<-EOF
  #!/bin/bash
  echo "Hello, World" > index.html
  nohup busybox httpd -f -p ${var.server_port} &
  EOF
----

== Data Sources

Data sources are provider-specific information fetched every time Terraform is run. Definition of the data source with the name "aws_vpc" to look up data for the default VPC:

[source,hcl-terraform]
----
data "aws_vpc" "default" {
  default = true
}
----

Arguments in data sources act as search filters.

Usage: Datasource "aws_subnets" uses data source "aws_vpc" and is used to define an auto-scaling group:

[source,hcl-terraform]
----
data "aws_subnets" "default" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.default.id]
  }
}

resource "aws_autoscaling_group" "example" {
  launch_configuration = aws_launch_configuration.example.name
  vpc_zone_identifier  = data.aws_subnets.default.ids

  min_size = 2
  max_size = 10

  tag {
    key                 = "Name"
    value               = "terraform-asg-example"
    propagate_at_launch = true
  }
}
----

== Terraform State

Terraform state is held as JSON in _terraform.tfstate_-file.

State should never be edited outside of Terraform!

State should only be stored locally on the development machine for small test projects. Normally, it is *shared* between all team members that need to work with Terraform. As soon as state is shared, a *locking mechanism* needs to be established so that not multile Terraform operations are execeuted in parallel.

Although Terraform code should be stored in version control like Git, Terraform state should *not* be stored there because of

* forgetting to pull the most recent state bevor executing operations with the outdated state,
* no locking mechanism and
* secrets would also have to be shared in Git.

Solution to these problems: *Terraform built-in support for remote Backends*:

* default backend = local backend
* remote backends allow storing state remotely, for example S3, Azure Storage, Terraform Enterprise.
* most recent state automatically loaded by Terraform
* Terraform automatically acquires a lock when executing _apply_
* support to store secrets encrypted remotely

When using AWS, state is best managed with S3 because of

* durable, available, inexpensive managed service
* supports encryption
* supports versioning

=== Recommended Combination: S3 + Dynamo

First, create S3 to store state and the Dynamo to keep the lock:

[source,hcl-terraform]
----
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.8"
    }
  }
}

provider "aws" {
  region = "us-east-2"
}

resource "aws_s3_bucket" "terraform_state" {

  bucket = "tf-state-development"

  # don't delete this bucket when running "terraform destroy"
  lifecycle {
    prevent_destroy = true
  }
}

# Explicitly block all public access to the S3 bucket
resource "aws_s3_bucket_public_access_block" "public_access" {
  bucket                  = aws_s3_bucket.terraform_state.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# version all changes
resource "aws_s3_bucket_versioning" "versioning_example" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

# server-side encryption
resource "aws_s3_bucket_server_side_encryption_configuration" "example" {
  bucket = aws_s3_bucket.terraform_state.bucket

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "AES256"
    }
  }
}

resource "aws_dynamodb_table" "tf-locks-development" {
  name = "tf-locks-development"
  billing_mode = "PAY_PER_REQUEST"
  hash_key = "LockID"
  attribute {
    name = "LockID"
    type = "S"
  }
}
----

Note: Primary key has to be named "LockID"!

Finally, configure Terraform to use the new backend:

[source,hcl-terraform]
----
terraform {
  backend "s3" {
    bucket = "tf-state-development"
    key = "global/s3/terraform.tfstate"
    region = "us-east-2"
    dynamodb_table = "tf-locks-development"
    encrypt = true
  }
}
----

After creating these files, a *terraform init* has to be performed to move the state from local to remote.

Recommended output variables:

[source,hcl-terraform]
----
output "s3_bucket_arn" {
  value = aws_s3_bucket.tf-state-development.arn
  description = "The ARN of the S3 bucket"
}
output "dynamodb_table_name" {
  value = aws_dynamodb_table.tf-locks-development.name
  description = "The name of the DynamoDB table"
}
----

=== Use Partial Configuration to Avoid Having to Copy-Paste when Using Backends

*A major drawback* of using Terraform backends is that *no variables may be used* in the _backend_-part of the configuration. Hence, the bucket name, region and dynamo table name have to be copied by hand to every module, see https://www.terraform.io/language/settings/backends/configuration["A backend block cannot refer to named values (like input variables, locals, or data source attributes)."]

Small alleviation for this inconvenience: *partial configuration* = omit some configuration and add it via _-backend-config_ when executing _terraform init_. All repeated arguments could be extracted into a file like _backend.hcl_ like this:

.backend.hcl
[source,hcl-terraform]
----
bucket = "terraform-up-and-running-state"
region = "us-east-2"
dynamodb_table = "terraform-up-and-running-locks"
encrypt = true
----

The _key_-value however must be included in the module because each module has its own key.

_backend.hcl_ could be used with:

[source,terminal]
----
$ terraform init -backend-config=backend.hcl
----

=== Isolation of State Files

Terraform needs a valid state to work properly. Hence, the integrity of this state has to be preserved at all times, as https://charity.wtf/2016/03/30/terraform-vpc-and-why-you-want-a-tfstate-file-per-env/[the nicely-written article "Terraform, VPC, And Why You Want A TFState File Per Env"] shows.

To keep problems isolated to, *multiple terraform files and states should be used* instead of just one.

General recommendation for slicing of Terraform files:

* *Separate staging* (dev, qa, prod). Use one set of Terraform files for each stage so that they live in *their own directory* with their own state files. *Using workspaces for this is not enough* because confusing workspaces on the command line is easy because the user has to explicitly choose the correct workspace. Also, all workspaces share the same S3 bucket to store their state in. Hence, the Terraform files have to live in separate directories. For each of these files, a *different backend* has to be configured, using different credentials. In AWS, this can be achieved by using *different AWS accounts for the stages*.
* *Separate components* like VPCs, services and databases so that changes on one don't pose a risk on the other. This is also done with *separating directories*.

Additionally, the Terraform files themselves should be organized into different (Git) repositories:

* The *module repository* is a library of project-specific resources that are typically deployed together, for example a "backend-server" implemented with AWS ECS or a "frontend-server" composed out of AWS CloudFront and S3.
* The *live repository* uses the modules from the module repository to create the different stages.

Example hierarchy of the *module repository*:

[source,terminal]
----
.
├── modules
│   ├── ecs-server
│   ├── pipeline
│   │   └── create-state-resources
│   │       ├── create-state-resources.tf
│   │       └── terraform.tfstate
│   ├── readme.adoc
│   └── shared-vpc
└── readme.adoc
----

Example hierarchy of the *live repository*:

[source,terminal]
----
.
├── dev
│   ├── 01-network
│   │   ├── network.tf
│   │   └── readme.adoc
│   ├── 02-backend-server
│   │   ├── backend-server.tf
│   │   └── readme.adoc
│   └── 03-frontend-server
├── prod
│   ├── 01-network
│   │   ├── network.tf
│   │   └── readme.adoc
│   ├── 02-backend-server
│   │   ├── backend-server.tf
│   │   └── readme.adoc
│   └── 03-frontend-server
└── readme.adoc
----

The code for creating the S3 bucket and Dynamo for keeping the remote Terraform state are located in the modules repository in the _create-state-resources_ folder.

All terraform files in all stages can use common _variables.tf_ and _initialize.tf_ files that are located in a *common root directory*, accessed via symlinks or relative paths.

==== Versioning of Modules Repository

The live repository references the modules from the module repository via relative paths. Because the modules repository will have different versions with different resources, it is important to reference the correct version from the live repository. For example, a new version of the resources should be tested in a _dev_ stage but not immediately used in the _prod_ stage.

Although Terraform modules have not been introduced yet, here's how they are referenced from the root module using relative paths:

[source,hcl-terraform]
----
provider "aws" {
region = "us-east-2"
}

module "webserver_cluster" {
source = "../../../modules/ecs-server/server"
}
----

This approach does not allow for multiple stages referencing different versions of the modules repository. This can be solved by versioning the modules repository with Git and referencing the commits in this Git repository.

A https://www.terraform.io/language/modules/sources[generic Git repository can be referenced with any URL that is accepted by the git checkout command]:

* selecting a *release*: _source = "git::https://example.com/vpc.git?ref=v1.2.0"_
* selecting a *commit* via SHA-1 hash: _source = "git::https://example.com/storage.git?ref=51d462976d84fdea54b47d80dcabbf680badcdb8"_
* selecting a *tag*: _source = "git::https://example.com/storage.git?ref=mytag"_

For easier reading, the rest of this workshop uses the relative file notation.

=== terraform_remote_state

Problem with isolating Terraform files from each other: Resource dependencies cannot be used over multiple files.

One solution: Read information from the Terraform state. The following example reads address and port of a previously created database to print them on a web page, provided by a simple web server:

[source,hcl-terraform]
----
user_data = <<EOF
#!/bin/bash
echo "Hello, World" >> index.html
echo "${data.terraform_remote_state.db.outputs.address}" >> index.html
echo "${data.terraform_remote_state.db.outputs.port}" >> index.html
nohup busybox httpd -f -p ${var.server_port} &
EOF
----

== Built-In Functions and the Terraform Console

Terraform offers built-in functions for

* numeric conversion
* string manipulation
* working with collections
* encoding
* filesystem
* data and time
* hash and crypto
* IP network
* type conversion

A full list https://www.terraform.io/language/functions[can be found here]

A good way to test these functions is via the Terraform console.

The following example shows how to start the console and format a number:

[source,terminal]
----
$ terraform console
> format("%.3f", 3.1465468)
"3.147"
>
----

The remote state example above included a shell script as _user_data_ directly in the Terraform file:

[source,hcl-terraform]
----
user_data = <<EOF
#!/bin/bash
echo "Hello, World" >> index.html
echo "${data.terraform_remote_state.db.outputs.address}" >> index.html
echo "${data.terraform_remote_state.db.outputs.port}" >> index.html
nohup busybox httpd -f -p ${var.server_port} &
EOF
----

With Terraform functions, this can be altered to:

.user-data.sh
[source,shell script]
----
#!/bin/bash

cat > index.html <<EOF
<h1>Hello, World</h1>
<p>DB address: ${db_address}</p>
<p>DB port: ${db_port}</p>
EOF

nohup busybox httpd -f -p ${server_port} &
----

[source,hcl-terraform]
----
user_data = templatefile("user-data.sh", {
    server_port = var.server_port
    db_address  = data.terraform_remote_state.db.outputs.address
    db_port     = data.terraform_remote_state.db.outputs.port
  })
----

When referencing files that live in modules (for example, if the _user-data.sh_ file would be located in a module), the relative path can be defined like this:

[source,hcl-terraform]
----
user_data = templatefile("${path.module}/user-data.sh", {
    server_port = var.server_port
    db_address  = data.terraform_remote_state.db.outputs.address
    db_port     = data.terraform_remote_state.db.outputs.port
  })
----

The path of the referenced _user-data.sh_ file can be prefixed with:

* _path.module_ for a path relative to the module
* _path.root_ for a path relative to the root module
* _path.cwd_ for a path relative to the current work directory (normally equal to _path.root_, but may differ)


== Terraform Modules

Every folder with Terraform files is a "module".

If _apply_ is executed in a module, it is called a *root module*.

Modules should not have a _provider_-section because these are defined by the user of the module (the root module).

Use a module:

[source,hcl-terraform]
----
provider "aws" {
  region = "us-east-2"
}

module "webserver_cluster" {
  source = "../../../modules/ecs-server/server"
}
----

After adding a module, _init_ has to be called!

To make the modules usable in different stages and even for different projects, they have to have dynamic values such as names, using input variables:

[source, hcl-terraform]
----
resource "aws_security_group" "alb" {
  name = "${var.cluster_name}-alb"
}

resource "aws_security_group_rule" "allow_http_inbound" {
  type              = "ingress"
  security_group_id = aws_security_group.alb.id

  from_port   = 80
  to_port     = 80
  protocol    = "tcp"
  cidr_blocks = ["0.0.0.0/0"]
}

resource "aws_security_group_rule" "allow_all_outbound" {
  type              = "egress"
  security_group_id = aws_security_group.alb.id

  from_port   = 0
  to_port     = 0
  protocol    = "-1"
  cidr_blocks = ["0.0.0.0/0"]
}
----

The backend configuration has to be modified, too:

[source,hcl-terraform]
----
data "terraform_remote_state" "db" {
  backend = "s3"

  config = {
    bucket = var.db_remote_state_bucket
    key    = var.db_remote_state_key
    region = "us-east-2"
  }
}
----

The above inputs have to be set with:

[source,hcl-terraform]
----
module "webserver_cluster" {
  source = "../../../modules/services/webserver-cluster"

  cluster_name           = "webservers-stage"
  db_remote_state_bucket = "(YOUR_BUCKET_NAME)"
  db_remote_state_key    = "stage/data-stores/mysql/terraform.tfstate"
}
----

=== Module Locals

In general, values should only be defined once, for example using input variables. This is also true for values in modules that need to be used in multiple places within the module, like port numbers or CIDR blocks. The best way of defining module-internal values are _module locals_ that only define values within the current module and cannot be overridden from the outside (like input variables):

[source,hcl-terraform]
----
locals {
  http_port    = 80
  any_port     = 0
  any_protocol = "-1"
  tcp_protocol = "tcp"
  all_ips      = ["0.0.0.0/0"]
}
----

Usage with "_local.variable_name_":

[source,hcl-terraform]
----
resource "aws_lb_listener" "http" {
  load_balancer_arn = aws_lb.example.arn
  port              = local.http_port
  protocol          = "HTTP"
...
----

=== Module Outputs

Output variables of modules can be accessed like this:

[source,hcl-terraform]
----
module.my_module.some_variable
----

== Managing Secrets

Most likely places where secrets are needed when working with Terraform:

1. Providers
2. Resources and data sources
3. State files and plan files

=== Providers

*Credentials should never be stored in Terraform files!*

[source,hcl-terraform]
----
provider "aws" {
  region = "us-east-2"

  # DO NOT DO THIS!!!
  access_key = "(ACCESS_KEY)"
  secret_key = "(SECRET_KEY)"
  # DO NOT DO THIS!!!
}
----

Good option for human users as well as machine users (CI/CD) is to store them as *environment variables*:

[source,shell script]
----
$  export AWS_ACCESS_KEY_ID=(MY_ACCESS_KEY_ID)
$  export AWS_SECRET_ACCESS_KEY=(MY_SECRET_ACCESS_KEY)
----

(Note leading space before _export_, disables adding this command to the shell's history.)

However, secrets must be stored somewhere on developer's computer, for example in a secret store.

Many CI/CD tools offer a way to enter credentials safely and expose them via environment variables at runtime.

=== Resources and Data Sources

[source,hcl-terraform]
----
resource "aws_db_instance" "example" {
  identifier_prefix   = "terraform-up-and-running"
  engine              = "mysql"
  allocated_storage   = 10
  instance_class      = "db.t2.micro"
  skip_final_snapshot = true
  db_name             = var.db_name

  # DO NOT DO THIS!!!
  username = "admin"
  password = "password"
  # DO NOT DO THIS!!!
}
----

==== Option: Environment Variables

First, create variables for the credentials:

[source,hcl-terraform]
----
variable "db_username" {
  description = "The username for the database"
  type        = string
  sensitive   = true
}

variable "db_password" {
  description = "The password for the database"
  type        = string
  sensitive   = true
}
----

Use variables:

[source,hcl-terraform]
----
resource "aws_db_instance" "example" {
  identifier_prefix   = "terraform-up-and-running"
  engine              = "mysql"
  allocated_storage   = 10
  instance_class      = "db.t2.micro"
  skip_final_snapshot = true
  db_name             = var.db_name

  username = var.db_username
  password = var.db_password
}
----

Set variables as environment variables:

[source,shell script]
----
$  export TF_VAR_db_username=(DB_USERNAME)
$  export TF_VAR_db_password=(DB_PASSWORD)
----

Explicitly document that the secrets have to be exported as environment variables! This is not obvious from the Terraform code alone.

==== Option: Encrypted Files

General idea: Encrypt secrets with a key, store cipher text in file and put that file in version control. Key is managed with Key Management Service (KMS) of cloud provider.

However, this approach is tedious and error-prone.

==== Option: Secret Stores

Secret stores are services offered by the cloud providers to store the secrets for you and integrate the secret-requests into where the secrets are needed. Examples are AWS Secrets Manager or HashiCorp Vault.

After creating a secret with the name _db-creds_ in JSON format, it can be referenced with

[source, hcl-terraform]
----
data "aws_secretsmanager_secret_version" "creds" {
  secret_id = "db-creds"
}
----

... and converted from JSON with

[source,hcl-terraform]
----
locals {
  db_creds = jsondecode(
    data.aws_secretsmanager_secret_version.creds.secret_string
  )
}

resource "aws_db_instance" "example" {
  identifier_prefix   = "terraform-up-and-running"
  engine              = "mysql"
  allocated_storage   = 10
  instance_class      = "db.t2.micro"
  skip_final_snapshot = true
  db_name             = var.db_name

  username = local.db_creds.username
  password = local.db_creds.password
}
----


=== State Files and Plan Files

*Every secret used by Terraform will end up as plain text in the state file!* The way how the secret was introduced into Terraform doesn't matter.

Hence, the backend files should be kept in a store that supports encryption, like AWS S3.


== Multi-Provider Setup

* provider = plugins written by community and cloud vendors
* Terraform core calls provider code (written in Go) via RPC which call the clouds via HTTP via their CLIs

Usual provider setup consists of two blocks. The first block declares which provider to use:

[source,hcl-terraform]
----
terraform {
  required_providers {
    aws = {
        source  = "hashicorp/aws"
        version = "~> 4.0"
    }
  }
}
----

Here,

* "aws" is the local name used for the provider in this module.
* "source" is the URL where Terraform downloads the provider in the format [<HOSTNAME>/]<NAMESPACE>/<TYPE>
* "version" for the version to install, details see https://www.terraform.io/language/expressions/version-constraints[Terraform documentation]

The second block configures that provider:

[source,hcl-terraform]
----
provider "aws" {
  region = "us-east-2"
}
----

=== Multiple AWS Regions

Aliases are used to differentiate between multiple providers:

[source,hcl-terraform]
----
provider "aws" {
    region = "us-east-2"
    alias  = "region_us_east_2"
}

provider "aws" {
    region = "us-west-1"
    alias  = "region_us_west_1"
}
----

Use that providers by adding _provider_ to a data source or resource definition:

[source,hcl-terraform]
----
data "aws_region" "region_1" {
  provider = aws.region_us_east_2
}

data "aws_region" "region_2" {
  provider = aws.region_us_west_1
}
----

[source,hcl-terraform]
----
resource "aws_instance" "region_1" {
  provider = aws.region_us_east_2

  # different AMI IDs
  ami           = "ami-0fb653ca2d3203ac1"
  instance_type = "t2.micro"
}

resource "aws_instance" "region_2" {
  provider = aws.region_us_west_1

  # different AMI IDs
  ami           = "ami-01f87c43e618bf8f0"
  instance_type = "t2.micro"
}
----

==== Detect Region-Specific AMI IDs

[source,hcl-terraform]
----
data "aws_ami" "ubuntu_region_1" {
  provider = aws.region_us_east_2

  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

data "aws_ami" "ubuntu_region_2" {
  provider = aws.region_us_west_1

  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "region_1" {
  provider = aws.region_us_east_2

  ami           = data.aws_ami.ubuntu_region_1.id
  instance_type = "t2.micro"
}

resource "aws_instance" "region_2" {
  provider = aws.region_us_east_2

  ami           = data.aws_ami.ubuntu_region_2.id
  instance_type = "t2.micro"
}
----

=== Multiple AWS Accounts

In general, Amazon recommends to use different accounts for different stages like development, quality assurance and production. These stages should be isolated from each other! However, sometimes it makes sense to deploy resources in multiple AWS accounts. *This couples these accounts together and should only be done rarely*.

To target multiple AWS accounts from one Terraform script, use the same procedure as above with multiple regions:

[source,hcl-terraform]
----
provider "aws" {
    region = "us-east-2"
    alias  = "parent"
}

provider "aws" {
  region = "us-east-2"
  alias  = "child"

  assume_role {
    role_arn = "arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME>"
  }
}
----

This can be used to set up resources and data sources like this:

[source,hcl-terraform]
----
data "aws_caller_identity" "parent" {
  provider = aws.parent
}

data "aws_caller_identity" "child" {
  provider = aws.child
}
----

To allow modules to be used in multiple accounts, _configuration aliases_ are used:

.module/main.tf
[source,hcl-terraform]
----
terraform {
  required_providers {
    aws = {
      source                = "hashicorp/aws"
      version               = "~> 4.0"
      configuration_aliases = [aws.parent, aws.child]
    }
  }
}
----

Configuration aliases don't create providers themselves, they only use configurations via a providers map:

.live/.main.tf
[source,hcl-terraform]
----
provider "aws" {
  region = "us-east-2"
  alias  = "parent"
}

provider "aws" {
  region = "us-east-2"
  alias  = "child"

  assume_role {
    role_arn = "arn:aws:iam::222222222222:role/OrganizationAccountAccessRole"
  }
}

module "multi_account_example" {
  source = "../../modules/multi-account"

  providers = {
    aws.parent = aws.parent
    aws.child  = aws.child
  }
}
----


=== Control Structures

*This section is only a stub and has to be completed.*

==== Loops

===== Loops with Count Parameter

Create three unique IAM users:

[source,hcl-terraform]
----
resource "aws_iam_user" "example" {
  count = 3
  name  = "neo.${count.index}"
}
----

Same example with better names:

[source,hcl-terraform]
----
variable "user_names" {
  description = "Create IAM users with these names"
  type        = list(string)
  default     = ["neo", "trinity", "morpheus"]
}

resource "aws_iam_user" "example" {
  count = length(var.user_names)
  name  = var.user_names[count.index]
}
----

Drawback of using _count_: The items in the list are represented by their position in the list. Deleting one item in the middle will move the last item instead of just deleting one!

===== Loops with for_each Expressions

Loop over lists, sets, maps. For example, create the three IAM users from above:

[source,hcl-terraform]
----
resource "aws_iam_user" "example" {
  for_each = toset(var.user_names)
  name     = each.value
}
----

In contrast to the version using _count_, this approach enables deletions of users, even in the middle of the list.

== Sources
* https://www.youtube.com/watch?v=SLB_c_ayRMo["Terraform Course - Automate your AWS cloud infrastructure"], YouTube, 2:20:57
* https://learn.hashicorp.com[HashiCorp Learn], tutorials from HashiCorp
* https://www.terraformupandrunning.com[Terraform: Up and Running, Third Edition by Yevgeniy Brikman (O’Reilly)]. Copyright 2022 Yevgeniy Brikman, 978-1-098-11674-3..














