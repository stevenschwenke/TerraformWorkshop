:toc:

= Terraform Workshop

This workshop builds heavily on the great book "Terraform Up & Running" by Yevgeniy (Jim) Brikman, which I highly recommend reading.

* https://www.terraform.io[terraform.io]: "Terraform is an open-source infrastructure as code software tool that provides a consistent CLI workflow to manage hundreds of cloud services. Terraform codifies cloud APIs into declarative configuration files."
* written in "HashiCorp Configuration Language" (HCL) in file with .tf extension
* support to cloud vendor via https://registry.terraform.io/browse/providers[providers], which have to be downloaded from the https://registry.terraform.io[registry] when used
* example: https://registry.terraform.io/providers/hashicorp/aws/latest/docs[Using AWS Provider]
* benefit of Terraform: syntax for each provider is the same, hence not the specifics of a provider have to be learned, they are abstracted away by Terraform scripts that look the same for each provider.

== Creating First Resources

Resource syntax:

[source,hcl-terraform]
----
resource "<provider>_<resource_type>" "name" {
  config options
}
----

* _<provider>_ will be "aws" if AWS is to be targeted
* _<resource_type>_ can also be found in the https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/ami[documentation]

For example:

.01-create-ec2/main.tf
[source,hcl-terraform]
----
# Create a VPC
resource "aws_vpc" "example" {
  cidr_block = "10.0.0.0/16"
}
----

Or:

.01-create-ec2/main.tf
[source,hcl-terraform]
----
# Create an EC2
resource "aws_instance" "my-instance" {
  # Ubuntu Server 20.04 LTS
  ami = "ami-0d527b8c289b4af7f"
  instance_type = "t2.micro"
}
----

**AMI-IDs are region-specific! When copy-pasting an ID from the web console, make sure to select the right region.**

In the directory where the above file is located, Terraform has to be initiated to download all the necessary providers into the folder _.terraform_:

[source,terminal]
----
$ terraform init

Initializing the backend...

Initializing provider plugins...
- Finding hashicorp/aws versions matching "~> 3.0"...
- Installing hashicorp/aws v3.74.3...
- Installed hashicorp/aws v3.74.3 (signed by HashiCorp)

Terraform has created a lock file .terraform.lock.hcl to record the provider
selections it made above. Include this file in your version control repository
so that Terraform can guarantee to make the same selections by default when
you run "terraform init" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running "terraform plan" to see
any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other
commands will detect it and remind you to do so if necessary.
----

The command _terraform init_ is idempotent, it can be run multiple times in the same directory.

**The automatically created file .terraform.lock.hcl should be added to version control!**

**The automatically created file terraform.tfstate is needed to manage the state and must not be deleted. Also, in includes sensitive information and should not be put in version control!**

Validate configuration with

[source,terminal]
----
$ terraform validate
Success! The configuration is valid.
----

List of all resources to be created with

[source,terminal]
----
terraform plan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following
symbols:
  + create

Terraform will perform the following actions:

  # aws_instance.my-instance will be created
  + resource "aws_instance" "my-instance" {
      + ami                                  = "ami-0d527b8c289b4af7f"
      + arn                                  = (known after apply)
      + associate_public_ip_address          = (known after apply)
      + availability_zone                    = (known after apply)
      + cpu_core_count                       = (known after apply)
      + cpu_threads_per_core                 = (known after apply)
      + disable_api_termination              = (known after apply)
      + ebs_optimized                        = (known after apply)
      + get_password_data                    = false
      + host_id                              = (known after apply)
      + id                                   = (known after apply)
      + instance_initiated_shutdown_behavior = (known after apply)
      + instance_state                       = (known after apply)
      + instance_type                        = "t2.micro"
      + ipv6_address_count                   = (known after apply)
      + ipv6_addresses                       = (known after apply)
      + key_name                             = (known after apply)
      + monitoring                           = (known after apply)
      + outpost_arn                          = (known after apply)
      + password_data                        = (known after apply)
      + placement_group                      = (known after apply)
      + placement_partition_number           = (known after apply)
      + primary_network_interface_id         = (known after apply)
      + private_dns                          = (known after apply)
      + private_ip                           = (known after apply)
      + public_dns                           = (known after apply)
      + public_ip                            = (known after apply)
      + secondary_private_ips                = (known after apply)
      + security_groups                      = (known after apply)
      + source_dest_check                    = true
      + subnet_id                            = (known after apply)
      + tags_all                             = (known after apply)
      + tenancy                              = (known after apply)
      + user_data                            = (known after apply)
      + user_data_base64                     = (known after apply)
      + vpc_security_group_ids               = (known after apply)

      + capacity_reservation_specification {
          + capacity_reservation_preference = (known after apply)

          + capacity_reservation_target {
              + capacity_reservation_id = (known after apply)
            }
        }

      + ebs_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + snapshot_id           = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }

      + enclave_options {
          + enabled = (known after apply)
        }

      + ephemeral_block_device {
          + device_name  = (known after apply)
          + no_device    = (known after apply)
          + virtual_name = (known after apply)
        }

      + metadata_options {
          + http_endpoint               = (known after apply)
          + http_put_response_hop_limit = (known after apply)
          + http_tokens                 = (known after apply)
          + instance_metadata_tags      = (known after apply)
        }

      + network_interface {
          + delete_on_termination = (known after apply)
          + device_index          = (known after apply)
          + network_interface_id  = (known after apply)
        }

      + root_block_device {
          + delete_on_termination = (known after apply)
          + device_name           = (known after apply)
          + encrypted             = (known after apply)
          + iops                  = (known after apply)
          + kms_key_id            = (known after apply)
          + tags                  = (known after apply)
          + throughput            = (known after apply)
          + volume_id             = (known after apply)
          + volume_size           = (known after apply)
          + volume_type           = (known after apply)
        }
    }

Plan: 1 to add, 0 to change, 0 to destroy.

─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run "terraform
apply" now.
----

Create resources specified in current folder with:

[source,terminal]
----
$ terraform apply
----

Multiple executions of _terraform apply_ will not create multiple resources because of the names of the created resources and the declarative approach.

After modifications, run _terraform plan_ and after that _terraform apply_.

== Destroying Resources

To destroy all resources defined in the files in the current directory and created by those resources, run:

[source,terminal]
----
$ terraform destroy
----

Also, resources existing in AWS but not defined in Terraform will be deleted when executing _Terraform apply_.

== Referencing Resources

References to other resources are done with the <resource_type>.<resource_name>.id like in this example:

.02-referencing-resources/main.tf
[source,hcl-terraform]
----
# Create a VPC and subnet
resource "aws_vpc" "vpc-1" {
  cidr_block = "10.0.0.0/16"
  tags = {
    Name = "production "
  }
}

resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.vpc-1.id
  cidr_block = "10.0.1.0/24"
  tags = {
    Name = "prod-subnet"
  }
}
----

Using the _Name_-tag will allow easy identification of the created resource in the AWS web console because the name of resources will be shown there, if the tag is _Name_ (with upper-case!).

Resources don't need to be declared in a specific order, Terraform figures out what to create first by itself. It creates a dependency graph from all the references that can be printed with:

[source,terminal]
----
$ terraform graph
----

Output of _terraform graph_ is written in DOT and can be converted to images using tools like https://dreampuf.github.io/GraphvizOnline/[Graphviz Online].


== User Data

When starting an EC2 instance, user data can be defined to run some script after creation of the instance.

.03-practice-project/main.tf
[source,hcl-terraform]
----
resource "aws_instance" "web-server-instance" {
  # Ubuntu Server 20.04 LTS
  ami = "ami-04505e74c0741db8d"
  instance_type = "t2.micro"
  availability_zone = "us-east-1a"
  key_name = "main-keypair"

  network_interface {
    device_index = 0
    network_interface_id = aws_network_interface.web-server-nic.id
  }

  user_data = <<-EOF
    #!/bin/bash
    sudo apt update -y
    sudo apt install apache2 -y
    sudo systemctl start apache2
    sudo bash -c 'echo your very first web server > /var/www/html/index.html'
    EOF
  tags = {
    Name = "web-server"
  }
}
----

== Output Variables

Within Terraform script, *output* can be defined like this (values from '_terraform state show aws_some_resource_'):

[source,hcl-terraform]
----
# Print public IP of server after creation
output "server_public_ip" {
  value = aws_eip.aws_eip.public_ip
  description ="The public IP of the server"
}
----

_output_ can be every Terraform expression.

As a best practice, outputs should reside in a file *outputs.tf*.

After _terraform apply_, *all* outputs can be printed again using

[source,terminal]
----
$ terraform output
----

*Certain* outputs can be printed using

[source,terminal]
----
$ terraform output server_public_ip
----

Output variables may contain a configuration with the following parameters:

* description
* sensitive (true, if the output should not be printed at the end of _apply_)
* depends_on (if Terraform cannot figure out the dependencies by itself)

Example for showing public IP address of server:

[source,hcl-terraform]
----
output "public_ip" {
  value = aws_instance.example.public_ip
  description = "The public IP address of the web server"
}
----

== Useful Commands and Settings

To automatically *confirm* changes:

[source,terminal]
----
$ terraform apply --auto-approve
----

*Show current state*:

[source,hcl-terraform]
----
$ terraform show
----

*List* all resources:

[source,terminal]
----
$ terraform state list
----

*Show details* about one of the resources:

[source,terminal]
----
$ terraform state show aws_some_resource
----

To *refresh* all states and run the outputs again (great for printing the IPs mentioned before):

[source,terminal]
----
$ terraform refresh
----

*Targeting* single resources instead of changing all the resources in a script:

[source,terminal]
----
$ terraform destroy -target aws_some_resource
$ terraform apply -target aws_some_resource
----

== Input Variables

To follow the DRY principle, code can be extracted into variables that have the following optional parameters:

* description
* default
* type (for example string, number, bool, list, map, set, object, tuple, any)
* validation (custom validation rules)
* sensitive (true / false; log variable or not)

For example, extract the CIDR block from the following code:

[source,hcl-terraform]
----
resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.prod-vpc.id
  cidr_block = "10.0.1.0/24"
  availability_zone = "us-east-1a"
  tags = {
    Name = "prod-subnet"
  }
}
----

The following code uses a *variable reference* and will ask for a value for _subnet_prefix_ when performing _terraform apply_:

[source,hcl-terraform]
----
variable "subnet_prefix" {
  description = "cidr block for subnet"
  type = string
}

resource "aws_subnet" "subnet-1" {
  vpc_id = aws_vpc.prod-vpc.id
  cidr_block = var.subnet_prefix
  availability_zone = "us-east-1a"
  tags = {
    Name = "prod-subnet"
  }
}
----

It will also ask for a value when destroying the resource, which doesn't matter so any value can be entered or none at all.

When creating the resources, the value could be assigned via a _default_ field in the definition of the variable.

Alternatively, it can be defined as a command line argument:

[source,terminal]
----
$ terraform apply -var "subnet_prefix=10.0.100.0/24"
----

However, best solution to define variables is via a separate file called *_terraform.tfvars_* in the same directory as the _.tf_ file:

.terraform.tfvars
[source,hcl-terraform]
----
subnet_prefix = "10.0.200.0/24"
----

When creating multiple variable files, the default name _terraform.tfvars_ cannot be used. References to variable files can be assigned with:

[source,terminal]
----
$ terraform apply --var-file example.tfvars
----

Example with combined types:

[source,hcl-terraform]
----
variable "list_numeric_example" {
  description = "An example of a numeric list in Terraform"
  type = list(number)
  default = [1, 2, 3]
}
----

Example with combined, structural types:

[source,hcl-terraform]
----
variable "object_example" {
  description = "An example of a structural type in Terraform"
  type = object({
    name = string
    age = number
    tags = list(string)
    enabled = bool
  })
  default = {
    name = "value1"
    age = 42
    tags = ["a", "b", "c"]
    enabled = true
  }
}
----

Using *interpolation*, a variable can also be used in the user data block:

[source,hcl-terraform]
----
user_data = <<-EOF
  #!/bin/bash
  echo "Hello, World" > index.html
  nohup busybox httpd -f -p ${var.server_port} &
  EOF
----

== Data Sources

Data sources are provider-specific information fetched every time Terraform is run. Definition of the data source with the name "aws_vpc" to look up data for the default VPC:

[source,hcl-terraform]
----
data "aws_vpc" "default" {
  default = true
}
----

Arguments in data sources act as search filters.

Usage: Datasource "aws_subnets" uses data source "aws_vpc" and is used to define an auto-scaling group:

[source,hcl-terraform]
----
data "aws_subnets" "default" {
  filter {
    name   = "vpc-id"
    values = [data.aws_vpc.default.id]
  }
}

resource "aws_autoscaling_group" "example" {
  launch_configuration = aws_launch_configuration.example.name
  vpc_zone_identifier  = data.aws_subnets.default.ids

  min_size = 2
  max_size = 10

  tag {
    key                 = "Name"
    value               = "terraform-asg-example"
    propagate_at_launch = true
  }
}
----

== Terraform State

Terraform state is held as JSON in _terraform.tfstate_-file.

State should never be edited outside of Terraform!

State should only be stored locally on the development machine for small test projects. Normally, it is *shared* between all team members that need to work with Terraform. As soon as state is shared, a *locking mechanism* needs to be established so that not multile Terraform operations are execeuted in parallel.

Although Terraform code should be stored in version control like Git, Terraform state should *not* be stored there because of

* forgetting to pull the most recent state bevor executing operations with the outdated state,
* no locking mechanism and
* secrets would also have to be shared in Git.

Solution to these problems: *Terraform built-in support for remote Backends*:

* default backend = local backend
* remote backends allow storing state remotely, for example S3, Azure Storage, Terraform Enterprise.
* most recent state automatically loaded by Terraform
* Terraform automatically acquires a lock when executing _apply_
* support to store secrets encrypted remotely

When using AWS, state is best managed with S3 because of

* durable, available, inexpensive managed service
* supports encryption
* supports versioning

=== Recommended Combination: S3 + Dynamo

First, create S3 to store state and the Dynamo to keep the lock:

[source,hcl-terraform]
----
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.8"
    }
  }
}

provider "aws" {
  region = "us-east-2"
}

resource "aws_s3_bucket" "terraform_state" {

  bucket = "tf-state-development"

  # don't delete this bucket when running "terraform destroy"
  lifecycle {
    prevent_destroy = true
  }
}

# Explicitly block all public access to the S3 bucket
resource "aws_s3_bucket_public_access_block" "public_access" {
  bucket                  = aws_s3_bucket.terraform_state.id
  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# version all changes
resource "aws_s3_bucket_versioning" "versioning_example" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

# server-side encryption
resource "aws_s3_bucket_server_side_encryption_configuration" "example" {
  bucket = aws_s3_bucket.terraform_state.bucket

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm     = "AES256"
    }
  }
}

resource "aws_dynamodb_table" "tf-locks-development" {
  name = "tf-locks-development"
  billing_mode = "PAY_PER_REQUEST"
  hash_key = "LockID"
  attribute {
    name = "LockID"
    type = "S"
  }
}
----

Note: Primary key has to be named "LockID"!

Finally, configure Terraform to use the new backend:

[source,hcl-terraform]
----
terraform {
  backend "s3" {
    bucket = "tf-state-development"
    key = "global/s3/terraform.tfstate"
    region = "us-east-2"
    dynamodb_table = "tf-locks-development"
    encrypt = true
  }
}
----

After creating these files, a *terraform init* has to be performed to move the state from local to remote.

Recommended output variables:

[source,hcl-terraform]
----
output "s3_bucket_arn" {
  value = aws_s3_bucket.tf-state-development.arn
  description = "The ARN of the S3 bucket"
}
output "dynamodb_table_name" {
  value = aws_dynamodb_table.tf-locks-development.name
  description = "The name of the DynamoDB table"
}
----

=== Use Partial Configuration to Avoid Having to Copy-Paste when Using Backends

*A major drawback* of using Terraform backends is that *no variables may be used* in the _backend_-part of the configuration. Hence, the bucket name, region and dynamo table name have to be copied by hand to every module, see https://www.terraform.io/language/settings/backends/configuration["A backend block cannot refer to named values (like input variables, locals, or data source attributes)."]

Small alleviation for this inconvenience: *partial configuration* = omit some configuration and add it via _-backend-config_ when executing _terraform init_. All repeated arguments could be extracted into a file like _backend.hcl_ like this:

[source,hcl-terraform]
----
bucket = "terraform-up-and-running-state"
region = "us-east-2"
dynamodb_table = "terraform-up-and-running-locks"
encrypt = true
----

The _key_-value however must be included in the module because each module has its own key.

_backend.hcl_ could be used with:

[source,terminal]
----
$ terraform init -backend-config=backend.hcl
----

=== Isolation of State Files

Terraform needs a valid state to work properly. Hence, the integrity of this state has to be preserved at all times, as https://charity.wtf/2016/03/30/terraform-vpc-and-why-you-want-a-tfstate-file-per-env/[the nicely-written article "Terraform, VPC, And Why You Want A TFState File Per Env"] shows.

To keep problems isolated to, *multiple terraform files and states should be used* instead of just one.

General recommendation for slicing of Terraform files:

* *Separate staging* (dev, qa, prod). Use one set of Terraform files for each stage so that they live in *their own directory* with their own state files. *Using workspaces for this is not enough* because confusing workspaces on the command line is easy because the user has to explicitly choose the correct workspace. Also, all workspaces share the same S3 bucket to store their state in. Hence, the Terraform files have to live in separate directories. For each of these files, a *different backend* has to be configured, using different credentials. In AWS, this can be achieved by using *different AWS accounts for the stages*.
* *Separate components* like VPCs, services and databases so that changes on one don't pose a risk on the other. This is also done with *separating directories*.

Additionally, the Terraform files themselves should be organized into different (Git) repositories:

* The *module repository* is a library of project-specific resources that are typically deployed together, for example a "backend-server" implemented with AWS ECS or a "frontend-server" composed out of AWS CloudFront and S3.
* The *live repository* uses the modules from the module repository to create the different stages.

Example hierarchy of the *module repository*:

[source,terminal]
----
.
├── modules
│   ├── ecs-server
│   ├── pipeline
│   │   └── create-state-resources
│   │       ├── create-state-resources.tf
│   │       └── terraform.tfstate
│   ├── readme.adoc
│   └── shared-vpc
└── readme.adoc
----

Example hierarchy of the *live repository*:

[source,terminal]
----
.
├── dev
│   ├── 01-network
│   │   ├── network.tf
│   │   └── readme.adoc
│   ├── 02-backend-server
│   │   ├── backend-server.tf
│   │   └── readme.adoc
│   └── 03-frontend-server
├── prod
│   ├── 01-network
│   │   ├── network.tf
│   │   └── readme.adoc
│   ├── 02-backend-server
│   │   ├── backend-server.tf
│   │   └── readme.adoc
│   └── 03-frontend-server
└── readme.adoc
----

The live repository references the modules from the module repository via relative paths.

The code for creating the S3 bucket and Dynamo for keeping the remote Terraform state are located in the modules repository in the _create-state-resources_ folder.

All terraform files in all stages can use common _variables.tf_ and _initialize.tf_ files that are located in a *common root directory*, accessed via symlinks or relative paths.



=== terraform_remote_state

Problem with isolating Terraform files from each other: Resource dependencies cannot be used over multiple files.

One solution: Read information from the Terraform state. The following example reads address and port of a previously created database to print them on a web page, provided by a simple web server:

[source,hcl-terraform]
----
user_data = <<EOF
#!/bin/bash
echo "Hello, World" >> index.html
echo "${data.terraform_remote_state.db.outputs.address}" >> index.html
echo "${data.terraform_remote_state.db.outputs.port}" >> index.html
nohup busybox httpd -f -p ${var.server_port} &
EOF
----



== TODO

* Add proof of concept / test-delete script from https://charity.wtf/2016/03/30/terraform-vpc-and-why-you-want-a-tfstate-file-per-env/[this article], section "2. tag your tf-owned resource, and hava a kill script". Test-run in save environment.

== Sources
* https://www.youtube.com/watch?v=SLB_c_ayRMo["Terraform Course - Automate your AWS cloud infrastructure"], YouTube, 2:20:57
* https://learn.hashicorp.com[HashiCorp Learn], tutorials from HashiCorp
* https://www.terraformupandrunning.com[Terraform: Up and Running, Third Edition by Yevgeniy Brikman (O’Reilly)]. Copyright 2022 Yevgeniy Brikman, 978-1-098-11674-3..














